# -*- coding: utf-8 -*-
"""udemy courses.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SCuNkq0QSpwqNly7yDX7_h0hPUYcoTDl
"""

# nltk.download('punkt')
  # nltk.download('stopwords')
  # nltk.download('wordnet')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from datetime import datetime, timezone
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

# load the dataset
df = pd.read_csv('/content/udemy_courses-raw.csv', encoding='ISO-8859-1')
df.head(5)

df.shape

df.info()

"""# **Data Preprocessing**"""

# check for dublicates
df['course_id'].nunique()

df[df.duplicated()]

df.drop_duplicates(inplace=True)

print(df.shape)
print(df['course_id'].nunique())

df.set_index('course_id', inplace=True)
df.head(5)

# check for date column
df['published_timestamp'] = pd.to_datetime(df['published_timestamp']).dt.tz_localize(None)
df.dtypes

# check for null values
df.isna().sum()

columns_detect_outliers = df.describe().columns
print(list(columns_detect_outliers))
for c in list(columns_detect_outliers):
  plt.boxplot(df[c],vert=False)
  plt.title(c)
  plt.show()

# detect and handle outliers by zscore
columns_detect_outliers = df.describe().columns
for column in columns_detect_outliers:
  upper = df[column].mean() + 3*df[column].std()
  lower = df[column].mean() - 3*df[column].std()
  df[column] = df[column].clip(lower = lower, upper = upper)
  df[column] = df[column].round().astype(int)

df.head()

"""# **Exploratory Data Analysis**"""

df.describe().round(2)

df['year'] = df['published_timestamp'].dt.year
df['month'] = df['published_timestamp'].dt.month_name()
df['day'] = df['published_timestamp'].dt.day_name()
df['profit'] = df['price'] * df['num_subscribers']

#Histograms
numeric_columns = df.select_dtypes(include='number').columns
for column in numeric_columns:
    plt.figure()  
    df[column].plot.hist()  
    plt.title(f'Histogram of {column}')  
    plt.xlabel(column)  
    plt.ylabel('Frequency')  
    plt.show()

df["subject"]

# # what is the subjects and their counts
subject_counts = df["subject"].value_counts().reset_index()
subject_counts.columns = ["subject", "count"]
subject_counts

# what is the subjects and their percentage
df["subject"].value_counts().plot(kind="pie", autopct='%1.0f%%')

# how many subscribers in each subject
subject_with_subscribers = df.groupby("subject")["num_subscribers"].sum().reset_index()
subject_with_subscribers.columns = ['subject', 'num_subscribers']
subject_with_subscribers

sns.barplot(x = 'subject', y = 'num_subscribers', data = subject_with_subscribers)
plt.title('subject with num of subscribers')
plt.ylabel('num_subscribers', fontsize=12)
plt.xlabel('subject', fontsize=12)
plt.xticks(rotation=45)
plt.show()

subject_mean_price = df.groupby("subject")["price"].mean().round(2).reset_index()
subject_mean_price.columns = ["subject", "price"]
subject_mean_price.sort_values(by="price", ascending=False)

subject_mean_price = df.groupby("subject")["content_duration"].mean().round(2).reset_index()
subject_mean_price.columns = ["subject", "content_duration"]
subject_mean_price.sort_values(by="content_duration")

df.pivot_table(index="subject", columns="is_paid", values="url" ,fill_value=0, margins=True, aggfunc="count")

sns.countplot(data= df , x= df["subject"], hue="is_paid")
plt.title('subject regarding to status of paid')
plt.ylabel('count', fontsize=12)
plt.xlabel('subject', fontsize=12)
plt.xticks(rotation=45)
plt.show()

sns.countplot(data= df , x= df["subject"], hue="level")
plt.title('subject regarding to levels')
plt.ylabel('count', fontsize=12)
plt.xlabel('subject', fontsize=12)
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(15,8))
sns.countplot(data= df , x= df["subject"], hue="year")
plt.title('subject regarding to years')
plt.ylabel('count', fontsize=12)
plt.xlabel('subject', fontsize=12)
plt.xticks(rotation=45)
plt.show()

sns.scatterplot(data=df, x="price", y="num_subscribers")
plt.title('price with num_subscribers')
plt.ylabel('num_subscribers', fontsize=12)
plt.xlabel('price', fontsize=12)
plt.show()

sns.scatterplot(data=df, x="content_duration", y="price")
plt.title('price with content_duration')
plt.ylabel('price', fontsize=12)
plt.xlabel('content_duration', fontsize=12)
plt.show()

num_subscribers_each_year = df.groupby("year")["num_subscribers"].sum().reset_index()
num_subscribers_each_year.columns = ["year", "num_subscribers"]
num_subscribers_each_year
fig = px.line(num_subscribers_each_year, x="year", y="num_subscribers")
fig.show()

"""# **ML Model to classify the subject of the course**"""

data = df.copy(deep=True)
data

"""*pre processing for machine learning*



"""

reference_date = datetime.now()
data['days'] = (reference_date - data['published_timestamp']).dt.days

# handle numeric columns
data['duration_category'] = pd.cut(data['content_duration'],bins=[0,1,3,7,12,20,np.inf],labels=['0:1','1:3','3:7','7:12','12:20','20+'], include_lowest=True)
data['price_category'] = pd.cut(data["price"],bins=[0,1,50,100,150,np.inf],labels=["Free","1:50","50:100",'100:150',"150+"], include_lowest=True)
data['num_lectures_category'] = pd.cut(data["num_lectures"],bins=[0,50,100,150,200],labels=["0:50","50:100",'100:150',"150:200"], include_lowest=True)
data['num_subscribers_category'] = pd.cut(data["num_subscribers"],bins = 5,labels=["very low","low", "median", "large", "very large"], include_lowest=True)
data['num_reviews_category'] = pd.cut(data["num_reviews"],bins = 5,labels=["very low","low", "median", "large", "very large"], include_lowest=True)
data['profit_category'] = pd.cut(data["profit"],bins = 5,labels=["very low","low", "median", "large", "very large"], include_lowest=True)
data['status'] = pd.cut(data["days"],bins = 5,labels=["recent", "new","median","old","very old"], include_lowest=True)
data.sample(10)

# drop useless columns
data.reset_index(drop=True, inplace=True)
data.drop(['url', 'published_timestamp' , 'year'	, 'month', 	'day', "content_duration", "price", "num_lectures", "num_subscribers", "num_reviews", "profit", "days"], axis='columns', inplace=True)
data.head()

import string
def remove_punctuation(text):
    punctuationfree="".join([i for i in text if i not in string.punctuation])
    return punctuationfree

data['course_title'] = data['course_title'].apply(lambda x:remove_punctuation(x))
data.head()

data['course_title'] = data['course_title'].apply(lambda x: x.lower())
data.head()

def tokenization(text):
    tokens = word_tokenize(text)
    return tokens

data['course_title']= data['course_title'].apply(lambda x: tokenization(x))
data.head()

stopwords = nltk.corpus.stopwords.words('english')
def remove_stopwords(text):
   output= [i for i in text if i not in stopwords]
   return output

data['course_title']= data['course_title'].apply(lambda x:remove_stopwords(x))
data.head()

wordnet_lemmatizer = WordNetLemmatizer()
def lemmatizer(text):
    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]
    return lemm_text

data['course_title']=data['course_title'].apply(lambda x:lemmatizer(x))
data.head()

data['course_title'] = data["course_title"].apply(lambda x : " ".join(x))
data.head()

X = data.drop('subject', axis = 1)
X

Y = data.loc[ : , ["subject"]]
Y["subject"] = Y["subject"].replace({"Web Development": 0 , "Business Finance": 1, "Graphic Design" : 2, "Musical Instruments": 3})
Y

X = pd.get_dummies(X, columns = ['is_paid', 'level', "duration_category", "price_category", "num_lectures_category", "num_subscribers_category", "num_reviews_category", "profit_category", "status"])
X

"""feature selection"""

rfc = RandomForestClassifier(n_estimators=100, random_state=42)
rfc.fit(X.drop('course_title', axis = 1), Y["subject"])
indeces_to_remove = np.where(rfc.feature_importances_ < 0.02)
indeces_to_remove = indeces_to_remove[0]+1
print(indeces_to_remove)
X.drop(X.columns[indeces_to_remove], axis=1, inplace=True)
X

"""train-test-split"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True , random_state=3)

vectorizer = TfidfVectorizer()
vector = vectorizer.fit_transform(X_train['course_title'])
course_title = pd.DataFrame(vector.toarray(), columns=vectorizer.get_feature_names_out())
X_train = pd.concat([course_title, X_train.drop('course_title', axis = 1).reset_index(drop=True)], axis=1)
X_train
vector = vectorizer.transform(X_test['course_title'])
course_title_test = pd.DataFrame(vector.toarray(), columns=vectorizer.get_feature_names_out())
X_test = pd.concat([course_title_test, X_test.drop('course_title', axis = 1).reset_index(drop=True)], axis=1)
X_test

"""**models**"""

#LogisticRegression
model = LogisticRegression(max_iter=1000, C=50)
model.fit(X_train, Y_train)

prediction_on_test_data = model.predict(X_test)
accuracy_on_test_data = accuracy_score(Y_test, prediction_on_test_data)
print(f"accurecy of test data = {accuracy_on_test_data}")

clf = DecisionTreeClassifier()
clf.fit(X_train, Y_train)

prediction_on_test_data = clf.predict(X_test)
accuracy_on_test_data = accuracy_score(Y_test, prediction_on_test_data)
print(f"accurecy of test data = {accuracy_on_test_data}")

rf = RandomForestClassifier(n_estimators=1000)
rf.fit(X_train, Y_train)

prediction_on_test_data = rf.predict(X_test)
accuracy_on_test_data = accuracy_score(Y_test, prediction_on_test_data)
print(f"accurecy of test data = {accuracy_on_test_data}")

xg = XGBClassifier()
xg.fit(X_train, Y_train)

prediction_on_test_data = xg.predict(X_test)
accuracy_on_test_data = accuracy_score(Y_test, prediction_on_test_data)
print(f"accurecy of test data = {accuracy_on_test_data}")

